{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u_nickname</th>\n",
       "      <th>u_url</th>\n",
       "      <th>m_content</th>\n",
       "      <th>m_content_url</th>\n",
       "      <th>m_content_id</th>\n",
       "      <th>m_images</th>\n",
       "      <th>m_videos</th>\n",
       "      <th>r_comment_num</th>\n",
       "      <th>g_publish_time</th>\n",
       "      <th>r_like_num</th>\n",
       "      <th>r_trans_num</th>\n",
       "      <th>verified</th>\n",
       "      <th>u_id</th>\n",
       "      <th>u_desc</th>\n",
       "      <th>u_fans</th>\n",
       "      <th>u_follow</th>\n",
       "      <th>u_login_time</th>\n",
       "      <th>u_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senator John Cornyn</td>\n",
       "      <td>https://twitter.com/JohnCornyn</td>\n",
       "      <td>\"A University of Southampton study suggests th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1247121007749607429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>231</td>\n",
       "      <td>2020-04-06 19:18:47</td>\n",
       "      <td>509</td>\n",
       "      <td>218</td>\n",
       "      <td>1</td>\n",
       "      <td>13218102.0</td>\n",
       "      <td>Mainly news I find interesting, pics, and opin...</td>\n",
       "      <td>294161</td>\n",
       "      <td>11996</td>\n",
       "      <td>2008-02-08 03:52:55</td>\n",
       "      <td>Austin, Texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Paul Joseph Watson</td>\n",
       "      <td>https://twitter.com/PrisonPlanet</td>\n",
       "      <td>\"A study published in March indicated that if ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1240429794057236483</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>224</td>\n",
       "      <td>2020-03-19 08:08:03</td>\n",
       "      <td>11594</td>\n",
       "      <td>3385</td>\n",
       "      <td>1</td>\n",
       "      <td>18643437.0</td>\n",
       "      <td>\"Watson is a brilliant polemicist.\" The Specta...</td>\n",
       "      <td>1108941</td>\n",
       "      <td>770</td>\n",
       "      <td>2009-01-06 04:04:23</td>\n",
       "      <td>London</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brian Stelter</td>\n",
       "      <td>https://twitter.com/brianstelter</td>\n",
       "      <td>\"America has had the best response to coronavi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1239359123587846144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>938</td>\n",
       "      <td>2020-03-16 09:13:35</td>\n",
       "      <td>1455</td>\n",
       "      <td>184</td>\n",
       "      <td>1</td>\n",
       "      <td>14515799.0</td>\n",
       "      <td>Anchor of @ReliableSources and @CNN's chief me...</td>\n",
       "      <td>781737</td>\n",
       "      <td>7075</td>\n",
       "      <td>2008-04-25 02:41:42</td>\n",
       "      <td>New York City</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Laurie Garrett</td>\n",
       "      <td>https://twitter.com/Laurie_Garrett</td>\n",
       "      <td>\"As many as 14% of recovered #coronavirus pati...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1243328270059405312</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>127</td>\n",
       "      <td>2020-03-27 08:05:34</td>\n",
       "      <td>970</td>\n",
       "      <td>829</td>\n",
       "      <td>1</td>\n",
       "      <td>299273962.0</td>\n",
       "      <td>Former Sr Fellow @CFR_org. Recipient of Pulitz...</td>\n",
       "      <td>237918</td>\n",
       "      <td>1818</td>\n",
       "      <td>2011-05-16 04:00:40</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tom Fitton</td>\n",
       "      <td>https://twitter.com/TomFitton</td>\n",
       "      <td>\"BIG GUY\" Biden implicated in RICO-style deal ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1319789796185772033</td>\n",
       "      <td>['http://pbs.twimg.com/media/ElDVv-aWkAAE1d3.j...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>435</td>\n",
       "      <td>2020-10-24 07:56:22</td>\n",
       "      <td>3789</td>\n",
       "      <td>1580</td>\n",
       "      <td>1</td>\n",
       "      <td>18266688.0</td>\n",
       "      <td>President, Judicial Watch. (These are my perso...</td>\n",
       "      <td>1184751</td>\n",
       "      <td>3225</td>\n",
       "      <td>2008-12-20 22:32:44</td>\n",
       "      <td>Washington, DC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            u_nickname                               u_url  \\\n",
       "0  Senator John Cornyn      https://twitter.com/JohnCornyn   \n",
       "1   Paul Joseph Watson    https://twitter.com/PrisonPlanet   \n",
       "2        Brian Stelter    https://twitter.com/brianstelter   \n",
       "3       Laurie Garrett  https://twitter.com/Laurie_Garrett   \n",
       "4           Tom Fitton       https://twitter.com/TomFitton   \n",
       "\n",
       "                                           m_content  m_content_url  \\\n",
       "0  \"A University of Southampton study suggests th...            NaN   \n",
       "1  \"A study published in March indicated that if ...            NaN   \n",
       "2  \"America has had the best response to coronavi...            NaN   \n",
       "3  \"As many as 14% of recovered #coronavirus pati...            NaN   \n",
       "4  \"BIG GUY\" Biden implicated in RICO-style deal ...            NaN   \n",
       "\n",
       "          m_content_id                                           m_images  \\\n",
       "0  1247121007749607429                                                NaN   \n",
       "1  1240429794057236483                                                NaN   \n",
       "2  1239359123587846144                                                NaN   \n",
       "3  1243328270059405312                                                NaN   \n",
       "4  1319789796185772033  ['http://pbs.twimg.com/media/ElDVv-aWkAAE1d3.j...   \n",
       "\n",
       "  m_videos  r_comment_num       g_publish_time  r_like_num  r_trans_num  \\\n",
       "0      NaN            231  2020-04-06 19:18:47         509          218   \n",
       "1      NaN            224  2020-03-19 08:08:03       11594         3385   \n",
       "2      NaN            938  2020-03-16 09:13:35        1455          184   \n",
       "3      NaN            127  2020-03-27 08:05:34         970          829   \n",
       "4      NaN            435  2020-10-24 07:56:22        3789         1580   \n",
       "\n",
       "   verified         u_id                                             u_desc  \\\n",
       "0         1   13218102.0  Mainly news I find interesting, pics, and opin...   \n",
       "1         1   18643437.0  \"Watson is a brilliant polemicist.\" The Specta...   \n",
       "2         1   14515799.0  Anchor of @ReliableSources and @CNN's chief me...   \n",
       "3         1  299273962.0  Former Sr Fellow @CFR_org. Recipient of Pulitz...   \n",
       "4         1   18266688.0  President, Judicial Watch. (These are my perso...   \n",
       "\n",
       "    u_fans  u_follow         u_login_time          u_area  \n",
       "0   294161     11996  2008-02-08 03:52:55   Austin, Texas  \n",
       "1  1108941       770  2009-01-06 04:04:23          London  \n",
       "2   781737      7075  2008-04-25 02:41:42   New York City  \n",
       "3   237918      1818  2011-05-16 04:00:40    New York, NY  \n",
       "4  1184751      3225  2008-12-20 22:32:44  Washington, DC  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "import logging\n",
    "import tempfile\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from collections import OrderedDict\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "init_notebook_mode(connected=True) #do not miss this line\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "tweet_clean=pd.read_csv('/Users/kejinglin/Desktop/personal/mydata/tweet_clean.csv',encoding='utf-8')\n",
    "tweet_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"A University of Southampton study suggests the number of coronavirus cases could have been reduced by 95% had China moved to contain the virus three weeks sooner.\"',\n",
       " '\"A study published in March indicated that if Chinese authorities had acted three weeks earlier than they did, the number of coronavirus cases could have been reduced by 95% and its geographic spread limited.\"\\n\\nMedia: Boo hoo, stop being mean to China!',\n",
       " '\"America has had the best response to coronavirus in the world!\"\\n\\n—@mattgaetz on Fox',\n",
       " '\"As many as 14% of recovered #coronavirus patients in China have tested positive again...betwn 3 to 14% of patients were diagnosed w/ the coronavirus, after already being given the all-clear.\"\\nA 2nd wave coming?\\nAre #COVID19 tests failing?\\nSuperinfection? https://t.co/Jg3SsL94lc',\n",
       " '\"BIG GUY\" Biden implicated in RICO-style deal w/Chinese communists! Should @RealDonaldTrump trust FBI to investigate? Fauci emails show he approved WHO praise of China on #coronavirus transparency! Wray FBI Text Message Cover-up! @JudicialWatch Update! https://t.co/ThR2FvV1LH https://t.co/4vgFT6eNug',\n",
       " '\"China jaan gaya ki iski chappan inch ki nahi chaar inch ki chaati hai. Yeh Darpok aadmi hai\": @RahulGandhi said this on Feb 7, 2019\\n\\nRahul Gandhi has been prophetic. Every word that he has said has come true. https://t.co/y0K0IM0Bln',\n",
       " '\"China lied to the world.\" OK, maybe they did. I\\'m willing to believe that. But I also remember Iraq, Syria, Libya, Vietnam, Venezuela, Russiagate, impeachment, and many other US lies. Trump also lied initially about corona. So I reject the smug, self-assured moral grandstanding',\n",
       " '\"Chinese laboratories identified a mystery virus as a highly infectious new pathogen by late December last year, but they were ordered to stop tests, destroy samples and suppress the news.\"\\n\\nU.S. media: Stop being mean to China!\\n\\nhttps://t.co/wj0VyMdmZJ',\n",
       " '\"Corbella: Poilievre wants us to imagine how much better we\\'d be had borders been shut earlier\"\\nhttps://t.co/9t6BAmecbB',\n",
       " '\"Does anybody really believe this?\" At press conference, Trump slams China, Iran coronavirus stats https://t.co/GqUSqAuqy8']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus=[]\n",
    "a=[]\n",
    "for i in range(len(tweet_clean['m_content'])):\n",
    "        a=tweet_clean['m_content'][i]\n",
    "        corpus.append(a)\n",
    "        \n",
    "corpus[0:10]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder \"/var/folders/g8/fjfkltz11ml925dqpwv1hq740000gn/T\" will be used to save temporary dictionary and corpus.\n"
     ]
    }
   ],
   "source": [
    "TEMP_FOLDER = tempfile.gettempdir()\n",
    "print('Folder \"{}\" will be used to save temporary dictionary and corpus.'.format(TEMP_FOLDER))\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-19 13:54:51,330 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-02-19 13:54:51,446 : INFO : built Dictionary(18868 unique tokens: ['\"a', '95%', 'cases', 'china', 'contain']...) from 3363 documents (total 70162 corpus positions)\n",
      "2021-02-19 13:54:51,447 : INFO : saving Dictionary object under /var/folders/g8/fjfkltz11ml925dqpwv1hq740000gn/T/elon.dict, separately None\n",
      "2021-02-19 13:54:51,455 : INFO : saved /var/folders/g8/fjfkltz11ml925dqpwv1hq740000gn/T/elon.dict\n"
     ]
    }
   ],
   "source": [
    "# removing common words and tokenizing\n",
    "list1 = ['RT','rt']\n",
    "stoplist = stopwords.words('english') + list(punctuation) + list1\n",
    "\n",
    "texts = [[word for word in str(document).lower().split() if word not in stoplist] for document in corpus]\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save(os.path.join(TEMP_FOLDER, 'elon.dict'))  # store the dictionary, for future reference\n",
    "\n",
    "#print(dictionary)\n",
    "#print(dictionary.token2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-19 13:55:09,537 : INFO : storing corpus in Matrix Market format to /var/folders/g8/fjfkltz11ml925dqpwv1hq740000gn/T/elon.mm\n",
      "2021-02-19 13:55:09,540 : INFO : saving sparse matrix to /var/folders/g8/fjfkltz11ml925dqpwv1hq740000gn/T/elon.mm\n",
      "2021-02-19 13:55:09,541 : INFO : PROGRESS: saving document #0\n",
      "2021-02-19 13:55:09,571 : INFO : PROGRESS: saving document #1000\n",
      "2021-02-19 13:55:09,602 : INFO : PROGRESS: saving document #2000\n",
      "2021-02-19 13:55:09,633 : INFO : PROGRESS: saving document #3000\n",
      "2021-02-19 13:55:09,647 : INFO : saved 3363x18868 matrix, density=0.106% (67298/63453084)\n",
      "2021-02-19 13:55:09,648 : INFO : saving MmCorpus index to /var/folders/g8/fjfkltz11ml925dqpwv1hq740000gn/T/elon.mm.index\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'elon.mm'), corpus)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-19 13:55:35,330 : INFO : collecting document frequencies\n",
      "2021-02-19 13:55:35,330 : INFO : PROGRESS: processing document #0\n",
      "2021-02-19 13:55:35,346 : INFO : calculating IDF weights for 3363 documents and 18868 features (67298 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[corpus]  # step 2 -- use the model to transform vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-19 13:56:22,275 : INFO : using symmetric alpha at 0.14285714285714285\n",
      "2021-02-19 13:56:22,276 : INFO : using symmetric eta at 0.14285714285714285\n",
      "2021-02-19 13:56:22,280 : INFO : using serial LDA version on this node\n",
      "2021-02-19 13:56:22,296 : INFO : running online (single-pass) LDA training, 7 topics, 1 passes over the supplied corpus of 3363 documents, updating model once every 2000 documents, evaluating perplexity every 3363 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2021-02-19 13:56:22,297 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2021-02-19 13:56:22,298 : INFO : PROGRESS: pass 0, at document #2000/3363\n",
      "2021-02-19 13:56:23,723 : INFO : merging changes from 2000 documents into a model of 3363 documents\n",
      "2021-02-19 13:56:23,736 : INFO : topic #5 (0.143): 0.027*\"china\" + 0.011*\"coronavirus\" + 0.011*\"virus\" + 0.009*\"&amp;\" + 0.008*\"chinese\" + 0.007*\"world\" + 0.006*\"covid-19\" + 0.005*\"spread\" + 0.005*\"china.\" + 0.004*\"communist\"\n",
      "2021-02-19 13:56:23,738 : INFO : topic #4 (0.143): 0.036*\"china\" + 0.013*\"coronavirus\" + 0.006*\"us\" + 0.006*\"says\" + 0.006*\"trump\" + 0.004*\"virus\" + 0.004*\"covid\" + 0.004*\"&amp;\" + 0.004*\"cases\" + 0.004*\"chinese\"\n",
      "2021-02-19 13:56:23,739 : INFO : topic #2 (0.143): 0.025*\"china\" + 0.010*\"coronavirus\" + 0.008*\"virus\" + 0.007*\"&amp;\" + 0.006*\"us\" + 0.005*\"trump\" + 0.005*\"covid-19\" + 0.005*\"china.\" + 0.004*\"china,\" + 0.003*\"president\"\n",
      "2021-02-19 13:56:23,740 : INFO : topic #1 (0.143): 0.027*\"china\" + 0.011*\"&amp;\" + 0.009*\"coronavirus\" + 0.006*\"chinese\" + 0.006*\"people\" + 0.005*\"trump\" + 0.004*\"virus\" + 0.004*\"must\" + 0.004*\"#coronavirus\" + 0.004*\"world\"\n",
      "2021-02-19 13:56:23,742 : INFO : topic #0 (0.143): 0.011*\"china\" + 0.007*\"covid\" + 0.005*\"&amp;\" + 0.005*\"covid-19\" + 0.005*\"virus\" + 0.003*\"us\" + 0.003*\"china.\" + 0.003*\"global\" + 0.003*\"spread\" + 0.003*\"deaths\"\n",
      "2021-02-19 13:56:23,743 : INFO : topic diff=5.148031, rho=1.000000\n",
      "2021-02-19 13:56:24,693 : INFO : -11.014 per-word bound, 2067.8 perplexity estimate based on a held-out corpus of 1363 documents with 28745 words\n",
      "2021-02-19 13:56:24,693 : INFO : PROGRESS: pass 0, at document #3363/3363\n",
      "2021-02-19 13:56:25,403 : INFO : merging changes from 1363 documents into a model of 3363 documents\n",
      "2021-02-19 13:56:25,412 : INFO : topic #3 (0.143): 0.029*\"china\" + 0.015*\"coronavirus\" + 0.009*\"trump\" + 0.009*\"virus\" + 0.007*\"people\" + 0.006*\"us\" + 0.006*\"president\" + 0.005*\"chinese\" + 0.005*\"&amp;\" + 0.004*\"china.\"\n",
      "2021-02-19 13:56:25,413 : INFO : topic #5 (0.143): 0.030*\"china\" + 0.013*\"virus\" + 0.011*\"coronavirus\" + 0.011*\"&amp;\" + 0.008*\"world\" + 0.008*\"chinese\" + 0.007*\"china.\" + 0.006*\"covid-19\" + 0.006*\"•\" + 0.006*\"spread\"\n",
      "2021-02-19 13:56:25,414 : INFO : topic #0 (0.143): 0.008*\"china\" + 0.006*\"covid\" + 0.004*\"covid-19\" + 0.003*\"&amp;\" + 0.003*\"virus\" + 0.003*\"global\" + 0.002*\"china.\" + 0.002*\"deaths\" + 0.002*\"us\" + 0.002*\"spread\"\n",
      "2021-02-19 13:56:25,415 : INFO : topic #6 (0.143): 0.014*\"china\" + 0.008*\"coronavirus\" + 0.005*\"china,\" + 0.005*\"cases\" + 0.004*\"trump\" + 0.004*\"#coronavirus\" + 0.004*\"covid\" + 0.004*\"people\" + 0.003*\"covid-19\" + 0.003*\"#covid19\"\n",
      "2021-02-19 13:56:25,417 : INFO : topic #4 (0.143): 0.035*\"china\" + 0.013*\"coronavirus\" + 0.010*\"trump\" + 0.007*\"us\" + 0.006*\"covid\" + 0.006*\"says\" + 0.005*\"virus\" + 0.004*\"china,\" + 0.004*\"chinese\" + 0.004*\"&amp;\"\n",
      "2021-02-19 13:56:25,418 : INFO : topic diff=0.901467, rho=0.707107\n"
     ]
    }
   ],
   "source": [
    "total_topics = 7\n",
    "lda = models.LdaModel(corpus, id2word=dictionary, num_topics=total_topics)\n",
    "corpus_lda = lda[corpus_tfidf] # create a double wrapper over the original corpus: bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.008*\"china\" + 0.006*\"covid\" + 0.004*\"covid-19\" + 0.003*\"&amp;\" + 0.003*\"virus\" + 0.003*\"global\" + 0.002*\"china.\" + 0.002*\"deaths\" + 0.002*\"us\" + 0.002*\"spread\" + 0.002*\"china,\" + 0.002*\"account\" + 0.002*\"#covid19\" + 0.002*\"new\" + 0.002*\"government\" + 0.002*\"lockdown\" + 0.002*\"take\" + 0.002*\"relief\" + 0.002*\"even\" + 0.001*\"trump\"'),\n",
       " (1,\n",
       "  '0.027*\"china\" + 0.012*\"&amp;\" + 0.009*\"coronavirus\" + 0.008*\"trump\" + 0.007*\"chinese\" + 0.006*\"people\" + 0.006*\"virus\" + 0.005*\"must\" + 0.005*\"china.\" + 0.004*\"#coronavirus\" + 0.004*\"medical\" + 0.003*\"us\" + 0.003*\"world\" + 0.003*\"blame\" + 0.003*\"americans\" + 0.003*\"#china\" + 0.003*\"pandemic\" + 0.003*\"communist\" + 0.003*\"china’s\" + 0.003*\"virus.\"'),\n",
       " (2,\n",
       "  '0.025*\"china\" + 0.009*\"coronavirus\" + 0.009*\"virus\" + 0.008*\"trump\" + 0.007*\"&amp;\" + 0.007*\"us\" + 0.005*\"china.\" + 0.005*\"china,\" + 0.005*\"covid-19\" + 0.004*\"president\" + 0.003*\"one\" + 0.003*\"u.s.\" + 0.003*\"travel\" + 0.003*\"get\" + 0.002*\"crisis\" + 0.002*\"said\" + 0.002*\"coronavirus.\" + 0.002*\"it’s\" + 0.002*\"going\" + 0.002*\"chinese\"'),\n",
       " (3,\n",
       "  '0.029*\"china\" + 0.015*\"coronavirus\" + 0.009*\"trump\" + 0.009*\"virus\" + 0.007*\"people\" + 0.006*\"us\" + 0.006*\"president\" + 0.005*\"chinese\" + 0.005*\"&amp;\" + 0.004*\"china.\" + 0.004*\"#coronavirus\" + 0.004*\"new\" + 0.004*\"covid-19\" + 0.004*\"china’s\" + 0.004*\"cases\" + 0.004*\"american\" + 0.004*\"#covid19\" + 0.004*\"communist\" + 0.003*\"china,\" + 0.003*\"country\"'),\n",
       " (4,\n",
       "  '0.035*\"china\" + 0.013*\"coronavirus\" + 0.010*\"trump\" + 0.007*\"us\" + 0.006*\"covid\" + 0.006*\"says\" + 0.005*\"virus\" + 0.004*\"china,\" + 0.004*\"chinese\" + 0.004*\"&amp;\" + 0.004*\"cases\" + 0.003*\"one\" + 0.003*\"new\" + 0.003*\"pandemic\" + 0.003*\"president\" + 0.003*\"#coronavirus\" + 0.003*\"response\" + 0.003*\"world\" + 0.003*\"coronavirus.\" + 0.003*\"global\"'),\n",
       " (5,\n",
       "  '0.030*\"china\" + 0.013*\"virus\" + 0.011*\"coronavirus\" + 0.011*\"&amp;\" + 0.008*\"world\" + 0.008*\"chinese\" + 0.007*\"china.\" + 0.006*\"covid-19\" + 0.006*\"•\" + 0.006*\"spread\" + 0.006*\"communist\" + 0.004*\"health\" + 0.004*\"#coronavirus\" + 0.004*\"wuhan\" + 0.003*\"cases\" + 0.003*\"people\" + 0.003*\"first\" + 0.003*\"president\" + 0.003*\"china’s\" + 0.003*\"travel\"'),\n",
       " (6,\n",
       "  '0.014*\"china\" + 0.008*\"coronavirus\" + 0.005*\"china,\" + 0.005*\"cases\" + 0.004*\"trump\" + 0.004*\"#coronavirus\" + 0.004*\"covid\" + 0.004*\"people\" + 0.003*\"covid-19\" + 0.003*\"#covid19\" + 0.003*\"italy\" + 0.003*\"china.\" + 0.003*\"us\" + 0.003*\"virus\" + 0.003*\"want\" + 0.003*\"wuhan\" + 0.002*\"flu\" + 0.002*\"first\" + 0.002*\"confirmed\" + 0.002*\"korea\"')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show first n important word in the topics:\n",
    "lda.show_topics(total_topics,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lda = {i: OrderedDict(lda.show_topic(i,25)) for i in range(total_topics)}\n",
    "#data_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 74)\n"
     ]
    }
   ],
   "source": [
    "df_lda = pd.DataFrame(data_lda)\n",
    "df_lda = df_lda.fillna(0).T\n",
    "print(df_lda.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>china</th>\n",
       "      <th>covid</th>\n",
       "      <th>covid-19</th>\n",
       "      <th>&amp;amp;</th>\n",
       "      <th>virus</th>\n",
       "      <th>global</th>\n",
       "      <th>china.</th>\n",
       "      <th>deaths</th>\n",
       "      <th>us</th>\n",
       "      <th>spread</th>\n",
       "      <th>...</th>\n",
       "      <th>first</th>\n",
       "      <th>italy</th>\n",
       "      <th>want</th>\n",
       "      <th>flu</th>\n",
       "      <th>confirmed</th>\n",
       "      <th>korea</th>\n",
       "      <th>→</th>\n",
       "      <th>united</th>\n",
       "      <th>virus”</th>\n",
       "      <th>times</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007559</td>\n",
       "      <td>0.006396</td>\n",
       "      <td>0.003784</td>\n",
       "      <td>0.003286</td>\n",
       "      <td>0.003135</td>\n",
       "      <td>0.002594</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.002352</td>\n",
       "      <td>0.002316</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.026668</td>\n",
       "      <td>0.002415</td>\n",
       "      <td>0.002586</td>\n",
       "      <td>0.011895</td>\n",
       "      <td>0.005605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004575</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003441</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.024568</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004674</td>\n",
       "      <td>0.006991</td>\n",
       "      <td>0.008948</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.028526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003925</td>\n",
       "      <td>0.005234</td>\n",
       "      <td>0.008984</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.034541</td>\n",
       "      <td>0.005863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003516</td>\n",
       "      <td>0.005428</td>\n",
       "      <td>0.002565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002166</td>\n",
       "      <td>0.006891</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.029621</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006354</td>\n",
       "      <td>0.011036</td>\n",
       "      <td>0.012520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002839</td>\n",
       "      <td>0.006033</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.013581</td>\n",
       "      <td>0.003825</td>\n",
       "      <td>0.003486</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002622</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002754</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002235</td>\n",
       "      <td>0.003428</td>\n",
       "      <td>0.002599</td>\n",
       "      <td>0.002383</td>\n",
       "      <td>0.002128</td>\n",
       "      <td>0.002124</td>\n",
       "      <td>0.002101</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>0.00201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      china     covid  covid-19     &amp;     virus    global    china.  \\\n",
       "0  0.007559  0.006396  0.003784  0.003286  0.003135  0.002594  0.002395   \n",
       "1  0.026668  0.002415  0.002586  0.011895  0.005605  0.000000  0.004575   \n",
       "2  0.024568  0.000000  0.004674  0.006991  0.008948  0.000000  0.005022   \n",
       "3  0.028526  0.000000  0.003925  0.005234  0.008984  0.000000  0.004403   \n",
       "4  0.034541  0.005863  0.000000  0.003516  0.005428  0.002565  0.000000   \n",
       "5  0.029621  0.000000  0.006354  0.011036  0.012520  0.000000  0.006973   \n",
       "6  0.013581  0.003825  0.003486  0.000000  0.002622  0.000000  0.003183   \n",
       "\n",
       "     deaths        us    spread  ...     first     italy      want       flu  \\\n",
       "0  0.002352  0.002316  0.002099  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.003441  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.006910  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.005884  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.002166  0.006891  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "5  0.000000  0.002839  0.006033  ...  0.003156  0.000000  0.000000  0.000000   \n",
       "6  0.000000  0.002754  0.000000  ...  0.002235  0.003428  0.002599  0.002383   \n",
       "\n",
       "   confirmed     korea         →    united    virus”    times  \n",
       "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.00000  \n",
       "1   0.000000  0.000000  0.000000  0.000000  0.000000  0.00000  \n",
       "2   0.000000  0.000000  0.000000  0.000000  0.000000  0.00000  \n",
       "3   0.000000  0.000000  0.000000  0.000000  0.000000  0.00000  \n",
       "4   0.000000  0.000000  0.000000  0.000000  0.000000  0.00000  \n",
       "5   0.000000  0.000000  0.000000  0.000000  0.000000  0.00000  \n",
       "6   0.002128  0.002124  0.002101  0.002038  0.002024  0.00201  \n",
       "\n",
       "[7 rows x 74 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Iterable over raw text documents expected, string object received.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-9a129fcd1565>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# 所有的特征词，即关键词\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#     print(word)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0;31m# TfidfVectorizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1189\u001b[0m                 \u001b[0;34m\"Iterable over raw text documents expected, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m                 \"string object received.\")\n",
      "\u001b[0;31mValueError\u001b[0m: Iterable over raw text documents expected, string object received."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "import xlrd\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    corpus = all_words\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    word = vectorizer.get_feature_names()   # 所有的特征词，即关键词\n",
    "#     print(word)    \n",
    "    analyze = vectorizer.build_analyzer()  \n",
    "    weight = X.toarray()  \n",
    "#     print(weight)\n",
    "    \n",
    "    import numpy as np\n",
    "    import lda\n",
    "    \n",
    "    # 训练模型\n",
    "    model = lda.LDA(n_topics = 7, n_iter = 500, random_state = 1)\n",
    "    model.fit(np.asarray(weight))\n",
    "    \n",
    "    # 主题-词分布\n",
    "    topic_word = model.topic_word_  #生成主题以及主题中词的分布\n",
    "    #print(\"topic-word:\\n\", topic_word)\n",
    "    \n",
    "    # 计算topN关键词\n",
    "    n = 20  \n",
    "    for i, word_weight in enumerate(topic_word):  \n",
    "        #print(\"word_weight:\\n\", word_weight)\n",
    "        distIndexArr = np.argsort(word_weight)\n",
    "        #print(\"distIndexArr:\\n\", distIndexArr)\n",
    "        topN_index = distIndexArr[:-(n+1):-1]\n",
    "        #print(\"topN_index:\\n\", topN_index) # 权重最在的n个\n",
    "        topN_words = np.array(word)[topN_index]    \n",
    "        print(u'*Topic {}\\n- {}'.format(i, ' '.join(topN_words))) \n",
    "    \n",
    "    #绘制主题-词分布图\n",
    "    import matplotlib.pyplot as plt  \n",
    "    f, ax= plt.subplots(2, 1, figsize=(6, 6), sharex=True)  \n",
    "    for i, k in enumerate([0,6]):         #两个主题\n",
    "        ax[i].stem(topic_word[k,:], linefmt='b-',  \n",
    "                   markerfmt='bo', basefmt='w-')  \n",
    "        ax[i].set_xlim(-2,20)  \n",
    "        ax[i].set_ylim(0, 1)  \n",
    "        ax[i].set_ylabel(\"Prob\")  \n",
    "        ax[i].set_title(\"topic {}\".format(k))  \n",
    "    ax[1].set_xlabel(\"word\")  \n",
    "    plt.tight_layout()  \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
