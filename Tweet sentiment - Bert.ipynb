{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'bert-for-tf2'...\n",
      "remote: Enumerating objects: 118, done.\u001b[K\n",
      "remote: Counting objects: 100% (118/118), done.\u001b[K\n",
      "remote: Compressing objects: 100% (82/82), done.\u001b[K\n",
      "remote: Total 997 (delta 49), reused 78 (delta 24), pack-reused 879\u001b[K\n",
      "Receiving objects: 100% (997/997), 295.08 KiB | 443.00 KiB/s, done.\n",
      "Resolving deltas: 100% (559/559), done.\n",
      "Collecting py-params>=0.9.6\n",
      "  Downloading py-params-0.10.2.tar.gz (7.4 kB)\n",
      "Collecting params-flow>=0.8.0\n",
      "  Downloading params-flow-0.8.2.tar.gz (22 kB)\n",
      "Requirement already satisfied: numpy in ./opt/anaconda3/lib/python3.8/site-packages (from params-flow>=0.8.0->-r bert-for-tf2/requirements.txt (line 5)) (1.19.2)\n",
      "Requirement already satisfied: tqdm in ./opt/anaconda3/lib/python3.8/site-packages (from params-flow>=0.8.0->-r bert-for-tf2/requirements.txt (line 5)) (4.50.2)\n",
      "Building wheels for collected packages: py-params, params-flow\n",
      "  Building wheel for py-params (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for py-params: filename=py_params-0.10.2-py3-none-any.whl size=7911 sha256=31f0fa6a9526bcb46f30ba5b931f4c3fb5e0b599d114c618b605524b9debe8c3\n",
      "  Stored in directory: /Users/kejinglin/Library/Caches/pip/wheels/ac/26/e9/df16869ccbd4abf517f1ff3be9a2c7ee5c5980fc87eea04fb1\n",
      "  Building wheel for params-flow (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for params-flow: filename=params_flow-0.8.2-py3-none-any.whl size=19471 sha256=733ee0f3a22b8fb4df6d20486a1b4b5b75a0dbdbf391582a3b55f83907070159\n",
      "  Stored in directory: /Users/kejinglin/Library/Caches/pip/wheels/c7/f3/85/b8cf1d8bfe55dc2ece0f1fcd4e91d6f8fc7b59ff3fd75329e1\n",
      "Successfully built py-params params-flow\n",
      "Installing collected packages: py-params, params-flow\n",
      "Successfully installed params-flow-0.8.2 py-params-0.10.2\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.95-cp38-cp38-macosx_10_6_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 863 kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.95\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "!git clone https://github.com/kpe/bert-for-tf2.git\n",
    "!pip install -r bert-for-tf2/requirements.txt\n",
    "!pip install sentencepiece\n",
    "\n",
    "sys.path.append(\"bert-for-tf2/\")\n",
    "\n",
    "import bert\n",
    "from bert.model import BertModelLayer\n",
    "from bert.loader import params_from_pretrained_ckpt, load_stock_weights\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"/Users/kejinglin/Desktop/personal/apple-twitter-sentiment-texts.csv\")\n",
    "df_train.head()\n",
    "\n",
    "data = df_train['text'].values\n",
    "labels = df_train['sentiment'].values+1 # if there is -1 in labels, loss could be nan\n",
    "\n",
    "x_train_text, x_valid_text, y_train, y_valid = train_test_split(data, labels, test_size=0.10, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 128\n",
    "CLASS = 3\n",
    "MODEL_PATH = '/Users/kejinglin/Desktop/personal/uncased_L-12_H-768_A-12/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = FullTokenizer(MODEL_PATH + 'vocab.txt', do_lower_case=False)\n",
    "\n",
    "train_tokens = []\n",
    "for row in x_train_text:\n",
    "    train_tokens.append( [\"[CLS]\"] + tokenizer.tokenize(str(row)) + [\"[SEP]\"] )\n",
    "\n",
    "train_token_ids = list(map(tokenizer.convert_tokens_to_ids, train_tokens))\n",
    "train_token_ids = map(lambda tids: tids + [0] * (SEQ_LEN - len(tids)), train_token_ids)\n",
    "train_token_ids = np.array([np.array(xi) for xi in list(train_token_ids)])\n",
    "\n",
    "valid_tokens = []\n",
    "for row in x_valid_text:\n",
    "    valid_tokens.append( [\"[CLS]\"] + tokenizer.tokenize(str(row)) + [\"[SEP]\"] )\n",
    "\n",
    "valid_token_ids = list(map(tokenizer.convert_tokens_to_ids, valid_tokens))\n",
    "valid_token_ids = map(lambda tids: tids + [0] * (SEQ_LEN - len(tids)), valid_token_ids)\n",
    "valid_token_ids = np.array([np.array(xi) for xi in list(valid_token_ids)])\n",
    "\n",
    "x_train = train_token_ids\n",
    "x_valid = valid_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading 196 BERT weights from: /Users/kejinglin/Desktop/personal/uncased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x7fbe479b8070> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tbert/embeddings/token_type_embeddings\n",
      "\tbert/pooler/dense/bias\n",
      "\tbert/pooler/dense/kernel\n",
      "\tcls/predictions/output_bias\n",
      "\tcls/predictions/transform/LayerNorm/beta\n",
      "\tcls/predictions/transform/LayerNorm/gamma\n",
      "\tcls/predictions/transform/dense/bias\n",
      "\tcls/predictions/transform/dense/kernel\n",
      "\tcls/seq_relationship/output_bias\n",
      "\tcls/seq_relationship/output_weights\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ids (InputLayer)       [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "bert (BertModelLayer)        (None, 128, 768)          108890112 \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                49216     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 108,939,523\n",
      "Trainable params: 49,411\n",
      "Non-trainable params: 108,890,112\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "bert_params = params_from_pretrained_ckpt(MODEL_PATH)\n",
    "bert_layer = BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "bert_layer.apply_adapter_freeze()\n",
    "\n",
    "def create_model(max_seq_length, classes):\n",
    "    inputs = Input(shape=(max_seq_length,), dtype='int32', name='input_ids')\n",
    "    bert = bert_layer(inputs)\n",
    "    cls_out = Lambda(lambda seq: seq[:, 0, :])(bert)\n",
    "    dr_1 = Dropout(0.3)(cls_out)\n",
    "    fc_1 = Dense(64, activation=tf.nn.relu)(dr_1)\n",
    "    dr_2 = Dropout(0.3)(fc_1)\n",
    "    outputs = Dense(classes, activation='softmax')(dr_2)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model(SEQ_LEN, CLASS)\n",
    "model.build(input_shape=(None, SEQ_LEN))\n",
    "\n",
    "load_stock_weights(bert_layer, MODEL_PATH+\"bert_model.ckpt\")\n",
    "\n",
    "def flatten_layers(root_layer):\n",
    "    if isinstance(root_layer, keras.layers.Layer):\n",
    "        yield root_layer\n",
    "    for layer in root_layer._layers:\n",
    "        for sub_layer in flatten_layers(layer):\n",
    "            yield sub_layer\n",
    "\n",
    "for layer in flatten_layers(bert_layer):\n",
    "        if layer.name in [\"LayerNorm\", \"adapter-down\", \"adapter-up\"]:\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "\n",
    "bert_layer.embeddings_layer.trainable = False\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=0.00001), metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "92/92 [==============================] - 328s 3s/step - loss: 1.4971 - accuracy: 0.2282 - val_loss: 1.1277 - val_accuracy: 0.1779\n",
      "\n",
      "Epoch 00001: saving model to bert_fine-tuning.ckpt\n",
      "Epoch 2/4\n",
      "92/92 [==============================] - 314s 3s/step - loss: 1.2038 - accuracy: 0.3909 - val_loss: 0.9836 - val_accuracy: 0.6626\n",
      "\n",
      "Epoch 00002: saving model to bert_fine-tuning.ckpt\n",
      "Epoch 3/4\n",
      "92/92 [==============================] - 304s 3s/step - loss: 1.0781 - accuracy: 0.4774 - val_loss: 0.9134 - val_accuracy: 0.6687\n",
      "\n",
      "Epoch 00003: saving model to bert_fine-tuning.ckpt\n",
      "Epoch 4/4\n",
      "92/92 [==============================] - 262s 3s/step - loss: 1.0775 - accuracy: 0.4683 - val_loss: 0.8795 - val_accuracy: 0.6871\n",
      "\n",
      "Epoch 00004: saving model to bert_fine-tuning.ckpt\n"
     ]
    }
   ],
   "source": [
    "checkpointName = \"bert_fine-tuning.ckpt\"\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpointName,\n",
    "                                                  save_weights_only=True,\n",
    "                                                  verbose=1)\n",
    "\n",
    "history = model.fit(x_train, y_train, \n",
    "                    epochs=4, batch_size=16,\n",
    "                    validation_data=(x_valid, y_valid),\n",
    "                    verbose=1, callbacks=[cp_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
